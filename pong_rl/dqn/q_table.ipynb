{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", desc=None, map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create q-table. 16 possible states, 4 actions for each state. 16x4 matrix.\n",
    "def init_q_table(state_space: int, action_space: int) -> np.ndarray:\n",
    "    \"\"\"Initialize the q-table with shape (state_space, action_space).\"\"\"\n",
    "    q_table = np.zeros((state_space, action_space))\n",
    "    return q_table\n",
    "\n",
    "q_table = init_q_table(env.observation_space.n, env.action_space.n)\n",
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def greedy_policy(q_table: np.ndarray, state: int) -> int:\n",
    "    \"\"\"Select the action that maximizes the reward, starting from the given state.\"\"\"\n",
    "    return q_table[state].argmax()\n",
    "\n",
    "def epsilon_greedy_policy(q_table: np.ndarray, state: int, epsilon: float) -> int:\n",
    "    \"\"\"\n",
    "    Select a random action with probability epsilon, and the greedy action with\n",
    "    probability 1 - epsilon. When epsilon is high, explore more, and when epsilon is low,\n",
    "    exploit more.\n",
    "    \"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return greedy_policy(q_table, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class HyperParams:\n",
    "    # Train params.\n",
    "    n_train_episodes: int\n",
    "    learning_rate: float\n",
    "\n",
    "    # Eval params.\n",
    "    n_eval_episodes: int\n",
    "\n",
    "    # Env params.\n",
    "    max_steps: int\n",
    "    gamma: float\n",
    "\n",
    "    # Exploration params\n",
    "    max_epsilon: float\n",
    "    min_epsilon: float\n",
    "    decay_rate: float\n",
    "\n",
    "hyper_params = HyperParams(\n",
    "    n_train_episodes=10_000,\n",
    "    learning_rate=0.7,\n",
    "    n_eval_episodes=100,\n",
    "    max_steps=99,\n",
    "    gamma=0.95,\n",
    "    max_epsilon=1.0,\n",
    "    min_epsilon=0.05,\n",
    "    decay_rate=0.0005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_epsilon(episode: int, hyper_params: HyperParams) -> float:\n",
    "    \"\"\"Get the exponentially decayed epsilon value based on the episode.\"\"\"\n",
    "    decay = np.exp(-1 * hyper_params.decay_rate * episode)  # Decays e^-x.\n",
    "\n",
    "    # Only decay difference to keep epsilon above 0.\n",
    "    decayable_epsilon = hyper_params.max_epsilon - hyper_params.min_epsilon\n",
    "    return hyper_params.min_epsilon + (decayable_epsilon * decay)\n",
    "\n",
    "episodes = np.arange(hyper_params.n_train_episodes)\n",
    "epsilons = get_epsilon(episodes, hyper_params)\n",
    "\n",
    "plt.plot(episodes, epsilons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(q_table: np.ndarray, env: gym.Env, hyper_params: HyperParams) -> np.ndarray:\n",
    "    \"\"\"Train the q_table and return the updated q_table.\"\"\"\n",
    "    for episode in range(hyper_params.n_train_episodes):\n",
    "        # Decay epsilon based on episode.\n",
    "        epsilon = get_epsilon(episode, hyper_params)\n",
    "        state, info = env.reset()\n",
    "\n",
    "        for step in range(hyper_params.max_steps):\n",
    "            # Take action and recieve observation.\n",
    "            action = epsilon_greedy_policy(q_table, state, epsilon)\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # Update q-table using TD.\n",
    "            value_estimate = reward + (hyper_params.gamma * q_table[new_state].max())\n",
    "            q_table[state, action] += hyper_params.learning_rate * (value_estimate - q_table[state, action])\n",
    "\n",
    "            # Start a new episode.\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            \n",
    "            # Update the state tracker. Onwards!\n",
    "            state = new_state\n",
    "\n",
    "    return q_table\n",
    "\n",
    "q_table = train(q_table, env, hyper_params)\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_q_table(q_table) -> plt.Figure:\n",
    "    \"\"\"Display the q_table as a heatmap.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "    img = ax.imshow(q_table)\n",
    "    fig.colorbar(img)\n",
    "\n",
    "    ax.set_xlabel(\"actions\")\n",
    "    ax.set_ylabel(\"states\")\n",
    "\n",
    "    for i in range(q_table.shape[0]):\n",
    "        for j in range(q_table.shape[1]):\n",
    "            text = ax.text(\n",
    "                j, i,\n",
    "                s=round(q_table[i, j], 3),\n",
    "                ha=\"center\", va=\"center\", color=\"w\"\n",
    "            )\n",
    "        \n",
    "    return fig\n",
    "\n",
    "display_q_table(q_table);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_agent(q_table: np.ndarray, env: gym.Env, hyper_params: HyperParams) -> tuple[float, float]:\n",
    "    \"\"\"Evaluate the agent, returning the mean and standard deviation of the rewards.\"\"\"\n",
    "    episode_rewards = []\n",
    "    for episode in range(hyper_params.n_eval_episodes):\n",
    "        total_reward = 0\n",
    "\n",
    "        # Reset environment and get initial state.\n",
    "        state, info = env.reset()\n",
    "        \n",
    "        for step in range(hyper_params.max_steps):\n",
    "            # Inference: use greedy policy to get action.\n",
    "            action = greedy_policy(q_table, state)\n",
    "\n",
    "            # Take action and get reward and new state.\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # Add reward to total reward.\n",
    "            total_reward += reward\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "            # Move to new state.\n",
    "            state = new_state\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "    return np.mean(episode_rewards), np.std(episode_rewards)\n",
    "\n",
    "eval_agent(q_table, env, hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as mpl_ani\n",
    "from IPython.display import HTML\n",
    "\n",
    "def get_frames(q_table: np.ndarray, env: gym.Env) -> list[np.ndarray]:\n",
    "    \"\"\"Play a single episode and return all frames.\"\"\"\n",
    "    state, info = env.reset()\n",
    "\n",
    "    frames = [env.render()]\n",
    "\n",
    "    while True:\n",
    "        action = greedy_policy(q_table, state)\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        frames.append(env.render())\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "        state = new_state\n",
    "\n",
    "    env.close()\n",
    "    return frames\n",
    "\n",
    "def show_episode(q_table: np.ndarray, env: gym.Env) -> None:\n",
    "    \"\"\"Show the agent going through a single episode.\"\"\"\n",
    "    frames = get_frames(q_table, env)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    img = ax.imshow(frames[0])\n",
    "\n",
    "    def update(frame: int):\n",
    "        \"\"\"Update the image.\"\"\"\n",
    "        img.set_data(frames[frame])\n",
    "        return img\n",
    "    \n",
    "    anim = mpl_ani.FuncAnimation(fig=fig, func=update, frames=len(frames))\n",
    "    return HTML(anim.to_jshtml())\n",
    "\n",
    "show_episode(q_table, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n",
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = init_q_table(env.observation_space.n, env.action_space.n)\n",
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = HyperParams(\n",
    "    n_train_episodes=25_000,\n",
    "    learning_rate=0.7,\n",
    "    n_eval_episodes=100,\n",
    "    max_steps=99,\n",
    "    gamma=0.95,\n",
    "    max_epsilon=1.0,\n",
    "    min_epsilon=0.05,\n",
    "    decay_rate=0.0005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = train(q_table, env, hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_agent(q_table, env, hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_episode(q_table, env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
